{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "   <h1>Analyse d'un Grand Ensemble de Donn√©es de Ventes </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des biblioth√®ques n√©cessaires\n",
    "Le code importe plusieurs biblioth√®ques Python utilis√©es pour la gestion des fichiers, la manipulation des donn√©es et la gestion des types :\n",
    "\n",
    "- **`h5py`** : Permet de travailler avec des fichiers HDF5, un format utilis√© pour stocker des donn√©es num√©riques complexes.\n",
    "- **`os`** : Fournit une interface pour interagir avec le syst√®me d'exploitation (par exemple, manipulation des fichiers).\n",
    "- **`time`** : Utilis√© pour mesurer le temps d'ex√©cution des op√©rations.\n",
    "- **`typing`** : Permet de sp√©cifier les types de donn√©es dans le code (facilite la lisibilit√© et la gestion du code).\n",
    "- **`sqlite3`** : Utilis√© pour interagir avec une base de donn√©es SQLite.\n",
    "- **`pandas`** : Biblioth√®que essentielle pour la manipulation de donn√©es sous forme de DataFrame (tableaux bidimensionnels).\n",
    "- **`modin.pandas`** : C'est une version parall√®le de pandas qui peut acc√©l√©rer les op√©rations sur les DataFrames en tirant parti de plusieurs c≈ìurs de CPU ou d'un cluster de machines. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as hdf5\n",
    "import os\n",
    "import time\n",
    "from typing import Optional\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "# import modin.pandas as mpd\n",
    "\n",
    "# üìÇ Chemin du fichier CSV et colonnes √† utiliser\n",
    "path_file: str = \"sales_data.csv\"  \n",
    "use_cols: list[str] = ['customer_id', 'product_id', 'quantity', 'price'] \n",
    "\n",
    "# ‚öôÔ∏è Types de donn√©es pour une gestion efficace de la m√©moire\n",
    "dtypes: dict[str, str] = {\n",
    "    'customer_id': 'uint32',  \n",
    "    'product_id': 'uint16',   \n",
    "    'quantity': 'uint8',      \n",
    "    'price': 'float32',       \n",
    "}\n",
    "\n",
    "# üîß Configuration de la taille des chunks pour le traitement par lot\n",
    "fraction: float = 0.01 \n",
    "chunk_size_rows: int = 100000 \n",
    "\n",
    "# üìä DataFrames globaux pour stocker les donn√©es\n",
    "data: pd.DataFrame = pd.DataFrame()  \n",
    "data_transaction: pd.DataFrame = pd.DataFrame()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## les d√©corateurs :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Gestion des erreurs avec le d√©corateur `check_fun_error`\n",
    ">Ce d√©corateur est utilis√© pour capturer et g√©rer les erreurs qui surviennent lors de l'ex√©cution d'une fonction. Si une exception est lev√©e dans la fonction d√©cor√©e, elle sera intercept√©e, et un message d'erreur personnalis√© sera affich√©, suivi de l'arr√™t du programme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ°Ô∏è D√©corateur pour la gestion des erreurs\n",
    "def check_fun_error(fun):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return fun(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå An error occurred in function '{fun.__name__}': {str(e)}\")\n",
    "            exit(1)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚è≥ Mesure du temps d'ex√©cution avec le d√©corateur `timing`\n",
    ">Ce d√©corateur mesure et affiche le temps d'ex√©cution d'une fonction. Il est utile pour analyser les performances des fonctions qui peuvent √™tre co√ªteuses en termes de temps de calcul, en particulier lors du traitement de grandes quantit√©s de donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚è≥ D√©corateur pour mesurer le temps d'ex√©cution d'une fonction\n",
    "def timing(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time: float = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time: float = time.time()\n",
    "        print(f\"‚è±Ô∏è Temps d'ex√©cution de {func.__name__}: {end_time - start_time:.4f} secondes\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Connexion √† une base de donn√©es avec le d√©corateur `connect_to_db`\n",
    ">Ce d√©corateur facilite la gestion de la connexion √† une base de donn√©es SQLite. Il cr√©e une connexion, passe cette connexion et le curseur de la base de donn√©es aux fonctions qui en ont besoin, puis ferme la connexion √† la fin de l'ex√©cution de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîå D√©corateur pour g√©rer la connexion √† la base de donn√©es SQLite\n",
    "def connect_to_db(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            # √âtablissement de la connexion √† la base de donn√©es\n",
    "            connection = sqlite3.connect('sales.db')\n",
    "            cursor = connection.cursor()\n",
    "            kwargs['connection'] = connection  \n",
    "            kwargs['cursor'] = cursor  \n",
    "            print(\"üîå Connexion √† la base de donn√©es r√©ussie\")\n",
    "            \n",
    "            return func(*args, **kwargs)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Gestion des erreurs\n",
    "            print(f\"‚ùå Une erreur est survenue dans la fonction '{func.__name__}': {str(e)}\")\n",
    "        \n",
    "        finally:\n",
    "            # Fermeture de la connexion √† la base de donn√©es\n",
    "            if 'connection' in locals():\n",
    "                print(\"üîå Fermeture de la connexion √† la base de donn√©es\")\n",
    "                connection.commit()  \n",
    "                connection.close() \n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. √âchantillonnage et Sous-ensemble de Donn√©es\n",
    "- T√¢che :\n",
    "    - Charger un √©chantillon al√©atoire de `1 %` des lignes du fichier `sales_data.csv`.\n",
    "    - S√©lectionner uniquement les colonnes `customer_id`, `product_id`, `quantity`, et `price`.\n",
    "    - Sp√©cifier les types de donn√©es appropri√©s pour r√©duire la consommation de m√©moire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Les donn√©es ont √©t√© charg√©es avec succ√®s.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   customer_id  10000 non-null  uint32 \n",
      " 1   product_id   10000 non-null  uint16 \n",
      " 2   quantity     10000 non-null  uint8  \n",
      " 3   price        10000 non-null  float32\n",
      "dtypes: float32(1), uint16(1), uint32(1), uint8(1)\n",
      "memory usage: 107.6 KB\n",
      "Taille des donn√©es:  None\n"
     ]
    }
   ],
   "source": [
    "# üì• Chargement du fichier CSV en chunks (morceaux)\n",
    "@check_fun_error\n",
    "def load_file_with_chunks(path_file : str) -> None:\n",
    "    global data\n",
    "    data = pd.DataFrame()\n",
    "    try:\n",
    "        for ch in pd.read_csv(path_file, usecols=use_cols, dtype=dtypes ,  chunksize=10000):\n",
    "            sampled_chunk = ch.sample(frac=fraction, random_state=4)\n",
    "            data = pd.concat([data, sampled_chunk], ignore_index=True)\n",
    "    finally:\n",
    "        if len(data) > 0:\n",
    "            print(\"‚úÖ Les donn√©es ont √©t√© charg√©es avec succ√®s.\")\n",
    "        else:\n",
    "             print(\"‚ö†Ô∏è Un probl√®me est survenu, les donn√©es sont vides.\")\n",
    "            \n",
    "if len(data) == 0:\n",
    "    load_file_with_chunks(path_file=path_file)\n",
    "    \n",
    "print(\"Taille des donn√©es: \", data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conversion en Formats de Fichiers Efficaces\n",
    "- T√¢che :\n",
    "    - Convertir l'√©chantillon de donn√©es en formats Feather et Parquet.\n",
    "    - Comparer la taille des fichiers et mesurer le temps de chargement pour chaque format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "üìñ Lecture d'un fichier CSV\n",
      "‚è±Ô∏è Temps d'ex√©cution de read_files_feather_parquet_csv: 0.0778 secondes\n",
      "üìñ Lecture d'un fichier Parquet\n",
      "‚è±Ô∏è Temps d'ex√©cution de read_files_feather_parquet_csv: 0.0336 secondes\n",
      "üìñ Lecture d'un fichier Feather\n",
      "‚è±Ô∏è Temps d'ex√©cution de read_files_feather_parquet_csv: 0.0242 secondes\n",
      "############################################################\n",
      "üì¶ Conversion au format Feather\n",
      "‚è±Ô∏è Temps d'ex√©cution de convert_to_feather_or_parquet: 0.0032 secondes\n",
      "############################################################\n",
      "üì¶ Conversion au format Parquet\n",
      "‚è±Ô∏è Temps d'ex√©cution de convert_to_feather_or_parquet: 0.0075 secondes\n",
      "############################################################\n",
      "üìè Taille du fichier CSV: 205935.00 octets\n",
      "üìè Taille du fichier Feather: 110730.00 octets\n",
      "üìè Taille du fichier Parquet: 159180.00 octets\n",
      "üìÅ Le fichier CSV est le plus grand.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üîÑ Convertir les donn√©es en format Feather ou Parquet\n",
    "@check_fun_error\n",
    "@timing\n",
    "def convert_to_feather_or_parquet(type_convert: str) -> None:\n",
    "    global data\n",
    "    type_convert = type_convert.lower()\n",
    "    if type_convert == \"feather\":\n",
    "        print(\"üì¶ Conversion au format Feather\")\n",
    "        return data.to_feather('data.feather')\n",
    "    elif type_convert == \"parquet\":\n",
    "        print(\"üì¶ Conversion au format Parquet\")\n",
    "        return data.to_parquet('data.parquet')\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Format non pris en charge. Veuillez choisir entre 'feather' ou 'parquet'.\")\n",
    "\n",
    "\n",
    "# üìñ Lire les fichiers en diff√©rents formats (Feather, Parquet, CSV)\n",
    "@check_fun_error\n",
    "@timing\n",
    "def read_files_feather_parquet_csv(type_file: str) -> Optional[pd.DataFrame]:\n",
    "    type_file = type_file.lower()\n",
    "    data.to_csv('data.csv', index=False)  \n",
    "    if type_file == \"feather\":\n",
    "        print(\"üìñ Lecture d'un fichier Feather\")\n",
    "        return pd.read_feather(\"data.feather\")\n",
    "    elif type_file == \"parquet\":\n",
    "        print(\"üìñ Lecture d'un fichier Parquet\")\n",
    "        return pd.read_parquet(\"data.parquet\")\n",
    "    elif type_file == \"csv\":\n",
    "        print(\"üìñ Lecture d'un fichier CSV\")\n",
    "        return pd.read_csv(\"data.csv\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Type de fichier non pris en charge. Choisissez parmi 'feather', 'parquet', ou 'csv'.\")\n",
    "        return None\n",
    "\n",
    "# üìä Comparer les tailles de fichiers\n",
    "@check_fun_error\n",
    "def compare_size_files() -> None:\n",
    "\n",
    "    data.to_csv('data.csv', index=False)\n",
    "    file_size_csv = os.path.getsize('data.csv')\n",
    "    file_size_feather = os.path.getsize('data.feather')\n",
    "    file_size_parquet = os.path.getsize('data.parquet')\n",
    "\n",
    "    print(f\"üìè Taille du fichier CSV: {file_size_csv:.2f} octets\")\n",
    "    print(f\"üìè Taille du fichier Feather: {file_size_feather:.2f} octets\")\n",
    "    print(f\"üìè Taille du fichier Parquet: {file_size_parquet:.2f} octets\")\n",
    "\n",
    "    # Comparaison des tailles des fichiers\n",
    "    if file_size_csv > file_size_feather and file_size_csv > file_size_parquet:\n",
    "        print(\"üìÅ Le fichier CSV est le plus grand.\")\n",
    "    elif file_size_feather > file_size_csv and file_size_feather > file_size_parquet:\n",
    "        print(\"üìÅ Le fichier Feather est le plus grand.\")\n",
    "    elif file_size_parquet > file_size_csv and file_size_parquet > file_size_feather:\n",
    "        print(\"üìÅ Le fichier Parquet est le plus grand.\")\n",
    "    else:\n",
    "        print(\"üìÅ Plusieurs fichiers ont des tailles √©quivalentes.\")\n",
    "\n",
    "# Utilisation des fonctions\n",
    "print(\"#\" * 60)\n",
    "df = read_files_feather_parquet_csv(\"csv\")\n",
    "df = read_files_feather_parquet_csv(\"parquet\")\n",
    "df = read_files_feather_parquet_csv(\"feather\")\n",
    "\n",
    "print(\"#\" * 60)\n",
    "convert_to_feather_or_parquet(\"feather\")\n",
    "\n",
    "print(\"#\" * 60)\n",
    "convert_to_feather_or_parquet(\"parquet\")\n",
    "\n",
    "print(\"#\" * 60)\n",
    "compare_size_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilisation de HDF5\n",
    "- T√¢che :\n",
    "    - Cr√©er un fichier HDF5 et stocker l'√©chantillon de donn√©es dans une table appel√©e `sales_sample`.\n",
    "    - Ajouter une autre table contenant les transactions dont le prix est sup√©rieur √† `100 DH`.\n",
    "    - Lire les 5 premi√®res lignes de la table `sales_sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es sauvegard√©es avec succ√®s dans le fichier HDF5.\n",
      "[[7.39730000e+04 9.20000000e+02 3.00000000e+00 4.11279999e+02]\n",
      " [8.43620000e+04 2.06000000e+02 1.00000000e+01 2.34100006e+02]\n",
      " [2.46190000e+04 7.11200000e+03 5.00000000e+00 3.37130005e+02]\n",
      " [5.21000000e+03 3.92000000e+03 7.00000000e+00 1.04580002e+02]\n",
      " [7.42710000e+04 4.99200000e+03 2.00000000e+00 9.20000000e+01]]\n",
      "############################################################\n",
      "   customer_id  product_id  quantity       price\n",
      "0      73973.0       920.0       3.0  411.279999\n",
      "1      84362.0       206.0      10.0  234.100006\n",
      "2      24619.0      7112.0       5.0  337.130005\n",
      "3       5210.0      3920.0       7.0  104.580002\n",
      "4      74271.0      4992.0       2.0   92.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üíæ Sauvegarder les donn√©es dans un fichier HDF5\n",
    "@check_fun_error\n",
    "def file_hdf5(file_name: str, data_transaction_supp_100: pd.DataFrame) -> None:\n",
    "\n",
    "    try:\n",
    "        # Ouverture du fichier HDF5 en mode √©criture\n",
    "        with hdf5.File(file_name, \"w\") as hd5_file:\n",
    "            # Cr√©ation des datasets dans le fichier HDF5\n",
    "            hd5_file.create_dataset('sales_sample', data=data)\n",
    "            hd5_file.create_dataset('sales_high_transaction', data=data_transaction_supp_100)\n",
    "        \n",
    "        print(\"‚úÖ Donn√©es sauvegard√©es avec succ√®s dans le fichier HDF5.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # En cas d'erreur, afficher un message et sortir\n",
    "        print(f\"‚ùå Une erreur est survenue : {str(e)}\")\n",
    "\n",
    "# üìñ Lire les cinq premi√®res lignes du fichier HDF5\n",
    "@check_fun_error\n",
    "def read_first_five_rows_from_hdf5(file_name: str) -> None:\n",
    "    try:\n",
    "        # Ouverture du fichier HDF5 en mode lecture\n",
    "        with hdf5.File(file_name, 'r') as hdf:\n",
    "            \n",
    "            sales_sample_data = hdf['sales_sample'][0:5]\n",
    "            print(sales_sample_data)\n",
    "            print('#' * 60)\n",
    "            df = pd.DataFrame(sales_sample_data , columns=data.columns)\n",
    "            print(df.head())\n",
    "    \n",
    "    except KeyError:\n",
    "        # Si le dataset n'existe pas dans le fichier\n",
    "        print(\"‚ùå Cl√© 'sales_sample' non trouv√©e dans le fichier HDF5.\")\n",
    "    except Exception as e:\n",
    "        # En cas d'erreur g√©n√©rale\n",
    "        print(f\"‚ùå Une erreur est survenue lors de la lecture du fichier HDF5 : {str(e)}\")\n",
    "\n",
    "\n",
    "data_transaction_supp_100 = data[data.price > 100]\n",
    "\n",
    "# Sauvegarde des donn√©es dans le fichier HDF5\n",
    "file_hdf5('sales_data.h5', data_transaction_supp_100=data_transaction_supp_100)\n",
    "\n",
    "# Lecture des cinq premi√®res lignes du fichier HDF5\n",
    "read_first_five_rows_from_hdf5('sales_data.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lecture par Morceaux\n",
    "- T√¢che :\n",
    "    - Lire le fichier `sales_data.csv` par morceaux de `100 000` lignes.\n",
    "    - Filtrer les transactions ayant une quantit√© sup√©rieure √† `10` pour chaque morceau.\n",
    "    - Combiner les r√©sultats filtr√©s dans un seul DataFrame et calculer la valeur totale des ventes `(quantit√© * prix)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Donn√©es (Premi√®res 100000 lignes) : \n",
      "     transaction_id  customer_id  product_id  quantity   price  \\\n",
      "0                1        15796         111         7  336.48   \n",
      "4                5         6266        7572         7   39.62   \n",
      "5                6        82387        9041         6   60.97   \n",
      "10              11        16024        7658         8   26.89   \n",
      "11              12        41091        3227         9  290.79   \n",
      "14              15          770        3597        10  149.93   \n",
      "16              17        62956        9581         7  380.04   \n",
      "21              22        53708        4655         8  376.55   \n",
      "23              24        28694        9645         7  338.83   \n",
      "25              26        93017        1958         7   81.08   \n",
      "\n",
      "   transaction_date         region  total_value  \n",
      "0        2023-10-03  North America      2355.36  \n",
      "4        2023-07-04  North America       277.34  \n",
      "5        2021-11-26  North America       365.82  \n",
      "10       2021-08-11      Australia       215.12  \n",
      "11       2020-09-22  South America      2617.11  \n",
      "14       2022-10-16  North America      1499.30  \n",
      "16       2023-08-10           Asia      2660.28  \n",
      "21       2020-01-04      Australia      3012.40  \n",
      "23       2023-08-07      Australia      2371.81  \n",
      "25       2020-09-27  North America       567.56  \n",
      "üíµ Valeur totale: 1021523122.1100003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üîÑ Traitement du fichier CSV par morceaux et renvoi des r√©sultats\n",
    "@check_fun_error\n",
    "def read_file_from_rows(file_path: str, chunk_size: int) -> pd.DataFrame:\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size): \n",
    "        yield chunk\n",
    "\n",
    "# üßÆ Combiner les transactions et calculer la valeur totale\n",
    "@check_fun_error\n",
    "def combine_and_calcul_total(min_qt: int) -> None:\n",
    "    global data_transaction\n",
    "    data_transaction = pd.DataFrame()\n",
    "\n",
    "    # Lecture du fichier par morceaux et filtrage\n",
    "    for chunk in read_file_from_rows('sales_data.csv', chunk_size=chunk_size_rows):\n",
    "     \n",
    "        data_transaction = pd.concat([data_transaction, chunk[chunk.quantity > min_qt]], ignore_index=False)\n",
    "\n",
    "    # Calcul de la valeur totale des transactions\n",
    "    data_transaction['total_value'] = data_transaction['price'] * data_transaction['quantity']\n",
    "    total = data_transaction['total_value'].sum()\n",
    "\n",
    "    # Affichage des 10 premi√®res lignes et du total\n",
    "    print(f\"üìä Donn√©es (Premi√®res {chunk_size_rows} lignes) : \\n\", data_transaction.head(10))\n",
    "    print(\"üíµ Valeur totale:\", total)\n",
    "\n",
    "\n",
    "# Exemple d'utilisation avec une quantit√© minimale de 5\n",
    "combine_and_calcul_total(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chargement dans une Base de Donn√©es SQLite\n",
    "- T√¢che :\n",
    "    - Cr√©er une base de donn√©es SQLite et charger l'int√©gralit√© du fichier `sales_data.csv` dans une table appel√©e `sales`.\n",
    "    - Ex√©cuter une requ√™te SQL pour extraire les transactions dans la r√©gion `Europe` avec un prix sup√©rieur √† `50 DH`.\n",
    "    - Calculer la valeur totale des ventes pour ces transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connexion √† la base de donn√©es r√©ussie\n",
      "‚úÖ Donn√©es CSV converties en base de donn√©es SQL avec succ√®s!\n",
      "üîå Fermeture de la connexion √† la base de donn√©es\n",
      "üîå Connexion √† la base de donn√©es r√©ussie\n",
      "üîå Fermeture de la connexion √† la base de donn√©es\n",
      "   transaction_id  customer_id  product_id  quantity   price transaction_date  \\\n",
      "0              59        67122        5281         9   23.44       2020-11-08   \n",
      "1              79        40398        5476         7   37.09       2022-10-01   \n",
      "2              83        55592        1249         9  336.71       2022-01-19   \n",
      "3              84        89813        7023        10   79.86       2023-05-26   \n",
      "4              94        39100        1501        10  318.90       2022-05-17   \n",
      "\n",
      "   region  total_value  \n",
      "0  Europe       210.96  \n",
      "1  Europe       259.63  \n",
      "2  Europe      3030.39  \n",
      "3  Europe       798.60  \n",
      "4  Europe      3189.00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from typing import Optional\n",
    "\n",
    "# üõ¢Ô∏è Convertir un fichier CSV en base de donn√©es SQL\n",
    "@check_fun_error\n",
    "@connect_to_db\n",
    "def convert_csv_to_sql_db(connection: sqlite3.Connection, cursor: Optional[sqlite3.Cursor] = None) -> None:\n",
    "\n",
    "    # Conversion des donn√©es en base de donn√©es SQL (Table 'sales')\n",
    "    data_transaction.to_sql('sales', connection, if_exists='replace', index=False)\n",
    "    print(\"‚úÖ Donn√©es CSV converties en base de donn√©es SQL avec succ√®s!\")\n",
    "\n",
    "\n",
    "# üîç S√©lectionner des transactions sp√©cifiques depuis la base de donn√©es\n",
    "@check_fun_error\n",
    "@connect_to_db\n",
    "def select_transaction_group_by_eur(cursor: sqlite3.Cursor, connection: Optional[sqlite3.Connection] = None) -> pd.DataFrame:\n",
    "\n",
    "    # Ex√©cution de la requ√™te SQL pour filtrer les transactions par r√©gion Europe\n",
    "    stm = cursor.execute(\"\"\"SELECT * FROM sales WHERE region='Europe' \"\"\")\n",
    "    # R√©cup√©ration des r√©sultats et conversion en DataFrame\n",
    "    rows = pd.DataFrame(stm.fetchall(), columns=data_transaction.columns)\n",
    "    return rows\n",
    "\n",
    "\n",
    "# Conversion du fichier CSV en base de donn√©es SQL\n",
    "convert_csv_to_sql_db()\n",
    "\n",
    "# S√©lectionner et afficher les transactions de la r√©gion 'Europe'\n",
    "rows = select_transaction_group_by_eur()\n",
    "print(rows.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R√©ponses aux Questions du TP\n",
    "### 1. √âchantillonnage et Sous-ensemble de Donn√©es\n",
    "- Pourquoi est-il utile de charger un √©chantillon al√©atoire de donn√©es plut√¥t que l'ensemble complet ?\n",
    "  - Permet d‚Äô√©conomiser la m√©moire.\n",
    "  - Acc√©l√®re les analyses.\n",
    "  - Permet de tester les scripts sur un sous-ensemble repr√©sentatif avant de travailler sur l‚Äôensemble des donn√©es.\n",
    "\n",
    "- Comment la sp√©cification des types de donn√©es peut-elle r√©duire la consommation de m√©moire ?\n",
    "  - En choisissant des types de donn√©es plus petits, comme int32, float32 ou category.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Conversion en Formats de Fichiers Efficaces\n",
    "- Quels sont les avantages des formats Feather et Parquet par rapport au format CSV ?\n",
    "  - **`Efficacit√© en taille`** : Feather et Parquet compressent les donn√©es, r√©duisant ainsi la taille des fichiers.\n",
    "  - **`Rapidit√©`** : Ces formats permettent une lecture et une √©criture plus rapides.\n",
    "  - **`Structure binaire`** : Mieux adapt√©s pour manipuler des donn√©es complexes avec des m√©tadonn√©es, contrairement au CSV, qui est un format texte.\n",
    "\n",
    "- Quand pr√©f√©rer Feather √† Parquet?\n",
    "  - **`Feather`** : Id√©al pour un usage rapide en Python, particuli√®rement avec pandas pour des manipulations en m√©moire.\n",
    "  - **`Parquet`** : Pr√©f√©r√© pour les projets n√©cessitant une compatibilit√© avec plusieurs outils (Hadoop, Spark) et une meilleure compression pour les donn√©es volumineuses.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Utilisation de HDF5\n",
    "-Qu'est-ce qu'un fichier HDF5 et comment est-il structur√© ?\n",
    "  - Un fichier HDF5 est un format binaire hi√©rarchique con√ßu pour organiser et stocker de grandes quantit√©s de donn√©es.\n",
    "  - Structure :\n",
    "    - **`Groupes`** : Similaires √† des dossiers.\n",
    "    - **`Datasets`** : Similaires √† des fichiers.\n",
    "\n",
    "-Pourquoi utiliser HDF5 plut√¥t qu'un fichier CSV ?\n",
    "  - **`Efficacit√©`** : Lecture/√©criture plus rapides pour de grands volumes de donn√©es.\n",
    "  - **`Compression`** : R√©duction de la taille des fichiers.\n",
    "  - **`Flexibilit√©`** : Supporte des structures complexes et permet d‚Äôacc√©der √† des parties sp√©cifiques sans charger tout le fichier.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Lecture par Morceaux\n",
    "- Pourquoi lire un fichier volumineux par morceaux ?\n",
    "  - Pour √©viter de d√©passer la capacit√© de la m√©moire vive (RAM).\n",
    "  - Permet un traitement progressif des donn√©es volumineuses.\n",
    "\n",
    "- Comment filtrer et combiner des donn√©es provenant de morceaux ?\n",
    "  - Utiliser une boucle avec pandas.read_csv() et le param√®tre chunksize.\n",
    "  - Appliquer des filtres sur chaque morceau.\n",
    "  - Combiner les r√©sultats √† l‚Äôaide de pd.concat().\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Chargement dans une Base de Donn√©es\n",
    "- Quels sont les avantages de stocker des donn√©es dans une base de donn√©es SQLite plut√¥t que dans un fichier CSV ?\n",
    "  - **`Requ√™tes complexes`** : Permet d‚Äôex√©cuter des requ√™tes SQL pour analyser efficacement les donn√©es.\n",
    "  - **`Structure`** : Organisation claire avec des types d√©finis.\n",
    "  - **`Performances`** : Acc√®s et filtrage des donn√©es sp√©cifiques plus rapides.\n",
    "\n",
    "- Comment ex√©cuter des requ√™tes SQL sur une base de donn√©es SQLite √† partir de Python ?\n",
    "  - Utiliser la biblioth√®que sqlite3 :\n",
    "    ```python\n",
    "    import sqlite3\n",
    "    \n",
    "    connection = sqlite3.connect('nom_base_donne.db')\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"Requete\")\n",
    "    connection.commit()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "   <h2>Mohamed BELANNAB </h2>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
