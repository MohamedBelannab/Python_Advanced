{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded successfully.\n",
      "############################################################\n",
      "You convert to feather \n",
      "Temps d'exécution de convert_to_feather_or_parquet: 0.0042 secondes\n",
      "############################################################\n",
      "You convert to parquet  \n",
      "Temps d'exécution de convert_to_feather_or_parquet: 0.0043 secondes\n",
      "############################################################\n",
      "############################################################\n",
      "############################################################\n",
      "Taille du fichier CSV: 57359485.00 octets\n",
      "Taille du fichier Feather: 6282.00 octets\n",
      "Taille du fichier Parquet: 6152.00 octets\n",
      "Le fichier CSV est le plus volumineux.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 111 entries, 959 to 230\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   customer_id  111 non-null    object \n",
      " 1   product_id   111 non-null    object \n",
      " 2   quantity     111 non-null    int16  \n",
      " 3   price        111 non-null    float32\n",
      "dtypes: float32(1), int16(1), object(2)\n",
      "memory usage: 3.3+ KB\n",
      "None\n",
      "    customer_id product_id  quantity       price\n",
      "959    CUST5339    PROD475         2  497.239990\n",
      "402    CUST6987    PROD365         6  575.200012\n",
      "845    CUST5815    PROD567        15  820.049988\n",
      "199    CUST8441    PROD778         7  709.280029\n",
      "932    CUST8652    PROD429         2  743.789978\n",
      "617    CUST7144    PROD457        16  819.239990\n",
      "643    CUST4101    PROD495        17  820.330017\n",
      "739    CUST1720    PROD528         4  440.250000\n",
      "821    CUST3173    PROD317         6  204.220001\n",
      "295    CUST8411    PROD256         4  563.390015\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "path_file = \"sales_data.csv\"\n",
    "use_cols = ['customer_id', 'product_id', 'quantity', 'price']\n",
    "\n",
    "dtypes = {          \n",
    "    'quantity': 'int16',          \n",
    "    'price': 'float32',          \n",
    "    'region': 'category'          \n",
    "}\n",
    "\n",
    "chunk_size = 1000\n",
    "data = pd.DataFrame() \n",
    "\n",
    "def check_fun_error(fun):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return fun(*args, **kwargs)  \n",
    "        except Exception as e: \n",
    "            print(f\"❌ An error occurred in function '{fun.__name__}': {str(e)}\")\n",
    "            exit(1)  \n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def timing(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Temps d'exécution de {func.__name__}: {end_time - start_time:.4f} secondes\")\n",
    "        return result\n",
    "    return wrapper\n",
    "            \n",
    "                 \n",
    "@check_fun_error  \n",
    "def load_file_with_chunks(path_file, chunk_size) -> None:\n",
    "    global data \n",
    "    try : \n",
    "        for ch in pd.read_csv(path_file, usecols= use_cols, dtype= dtypes  , chunksize=chunk_size):\n",
    "            data = pd.concat([data, ch], ignore_index=True).sample(frac=0.1  , random_state = 2)\n",
    "    finally : \n",
    "        if len(data) > 0:\n",
    "            print(\"✅ Data loaded successfully.\")\n",
    "        else:\n",
    "            print(\"⚠️ Something went wrong, data is empty.\")\n",
    "\n",
    "\n",
    "@check_fun_error\n",
    "@timing\n",
    "def convert_to_feather_or_parquet(type_convert) : \n",
    "    global data\n",
    "    if type_convert.lower() == \"feather\":\n",
    "        print(\"You convert to feather \")\n",
    "        return data.to_feather('data.feather')\n",
    "    if type_convert.lower() == \"parquet\" :\n",
    "        print(\"You convert to parquet  \") \n",
    "        return data.to_parquet('data.parquet')\n",
    "    \n",
    "@check_fun_error\n",
    "@timing\n",
    "def read_files_feather_parquet_csv(type_file):\n",
    "    type_file = type_file.lower()  \n",
    "    if type_file == \"feather\":\n",
    "        print(\"You are reading a Feather file\")\n",
    "        return pd.read_feather(\"data.feather\")\n",
    "    elif type_file == \"parquet\":\n",
    "        print(\"You are reading a Parquet file\")\n",
    "        return pd.read_parquet(\"data.parquet\")\n",
    "    elif type_file == \"csv\":\n",
    "        print(\"You are reading a CSV file\")\n",
    "        return pd.read_csv(\"sales_data.csv\")\n",
    "    else:\n",
    "        print(\"Unsupported file type. Please choose from 'feather', 'parquet', or 'csv'.\")\n",
    "        return None       \n",
    "\n",
    "@check_fun_error\n",
    "def compare_size_files():\n",
    "    # Obtenir la taille des fichiers en octets\n",
    "    file_size_csv = os.path.getsize('sales_data.csv')\n",
    "    file_size_feather = os.path.getsize('data.feather')\n",
    "    file_size_parquet = os.path.getsize('data.parquet')\n",
    "    \n",
    "    # Afficher les tailles des fichiers\n",
    "    print(f\"Taille du fichier CSV: {file_size_csv:.2f} octets\")\n",
    "    print(f\"Taille du fichier Feather: {file_size_feather:.2f} octets\")\n",
    "    print(f\"Taille du fichier Parquet: {file_size_parquet:.2f} octets\")\n",
    "\n",
    "    # Comparaison des tailles pour déterminer le fichier le plus volumineux\n",
    "    if file_size_csv > file_size_feather and file_size_csv > file_size_parquet:\n",
    "        print(\"Le fichier CSV est le plus volumineux.\")\n",
    "    elif file_size_feather > file_size_csv and file_size_feather > file_size_parquet:\n",
    "        print(\"Le fichier Feather est le plus volumineux.\")\n",
    "    elif file_size_parquet > file_size_csv and file_size_parquet > file_size_feather:\n",
    "        print(\"Le fichier Parquet est le plus volumineux.\")\n",
    "    else:\n",
    "        print(\"Plusieurs fichiers ont des tailles équivalentes.\")\n",
    "            \n",
    "        \n",
    "if len(data)  ==  0 :\n",
    "    load_file_with_chunks(path_file=path_file, chunk_size=chunk_size)\n",
    "\n",
    "data_transaction_supp_100 = data[ data.price > 100 ]\n",
    "    \n",
    "print(\"#\" * 60 )\n",
    "convert_to_feather_or_parquet(\"feather\")\n",
    "\n",
    "print(\"#\" * 60 )\n",
    "convert_to_feather_or_parquet(\"parquet\")\n",
    "\n",
    "print(\"#\" * 60 )\n",
    "# df = read_files_feather_parquet_csv(\"feather\")\n",
    "\n",
    "print(\"#\" * 60 )\n",
    "# df = read_files_feather_parquet_csv(\"parquet\")\n",
    "print(\"#\" * 60 )\n",
    "# df = read_files_feather_parquet_csv(\"csv\")\n",
    "\n",
    "# print(df.head(10))\n",
    "\n",
    "compare_size_files()\n",
    "\n",
    "print(data.info())\n",
    "print(data_transaction_supp_100.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = {\n",
    "    'TransactionID': [1, 2, 3, 4, 5],\n",
    "    'Produit': ['Produit A', 'Produit B', 'Produit C', 'Produit D', 'Produit E'],\n",
    "    'Prix': [50, 150, 80, 120, 200],\n",
    "    'Quantité': [1, 2, 1, 3, 1]\n",
    "}\n",
    "normal_data = pd.DataFrame(data)\n",
    "\n",
    "# Convertir les types avec numpy\n",
    "normal_data['TransactionID'] = normal_data['TransactionID'].astype(np.int32)\n",
    "normal_data['Produit'] = normal_data['Produit'].astype(np.str_)  \n",
    "normal_data['Prix'] = normal_data['Prix'].astype(np.float64)\n",
    "normal_data['Quantité'] = normal_data['Quantité'].astype(np.int32)\n",
    "\n",
    "# Filtrer les transactions dont le prix est supérieur à 100\n",
    "data_transaction_supp_100 = normal_data[normal_data['Prix'] > 100]\n",
    "\n",
    "# Convertir les DataFrame en un format structuré numpy pour HDF5\n",
    "dtype = np.dtype([('TransactionID', np.int32),\n",
    "                  ('Produit', np.string_, 100),  # Convertir les chaînes en np.string_ de longueur fixe\n",
    "                  ('Prix', np.float64),\n",
    "                  ('Quantité', np.int32)])\n",
    "\n",
    "# Convertir en tableau structuré\n",
    "normal_data_array = np.array(list(normal_data.itertuples(index=False, name=None)), dtype=dtype)\n",
    "data_transaction_supp_100_array = np.array(list(data_transaction_supp_100.itertuples(index=False, name=None)), dtype=dtype)\n",
    "\n",
    "# Créer un fichier HDF5 et y stocker les deux tables\n",
    "with h5py.File(\"sales_data.h5\", \"w\") as hd5_file:\n",
    "    hd5_file.create_dataset('sales_sample', data=normal_data_array)\n",
    "    hd5_file.create_dataset('sales_high_transaction', data=data_transaction_supp_100_array)\n",
    "\n",
    "with h5py.File('sales_data.h5', 'r') as hdf:\n",
    "    # Lire les données du table 'sales_sample'\n",
    "    print(hdf['sales_sample'][0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
