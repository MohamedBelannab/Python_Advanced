{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded successfully.\n",
      "Data : \n",
      "    transaction_id  customer_id  product_id  quantity   price transaction_date  \\\n",
      "0               2         6831         260        17  817.46       2023-02-28   \n",
      "1               3         8325         677        19  105.57       2020-10-11   \n",
      "2               4         5339         932        11   29.94       2024-07-17   \n",
      "3               5         3868         546        12   35.08       2022-07-05   \n",
      "4               6         9910         721        15  625.86       2024-06-30   \n",
      "5               7         4165         782        17   12.08       2023-06-26   \n",
      "6              12         2814         532        19  892.34       2021-06-22   \n",
      "7              13         2085         354        16  918.40       2021-12-09   \n",
      "8              15         2577         301        17   32.87       2023-10-08   \n",
      "9              16         3343         509        17  840.91       2020-11-14   \n",
      "\n",
      "    region  total_value  \n",
      "0  Central     13896.82  \n",
      "1    South      2005.83  \n",
      "2    South       329.34  \n",
      "3    South       420.96  \n",
      "4  Central      9387.90  \n",
      "5    North       205.36  \n",
      "6  Central     16954.46  \n",
      "7  Central     14694.40  \n",
      "8     East       558.79  \n",
      "9     East     14295.47  \n",
      "Total : \n",
      " 193211.71\n",
      "connect success\n",
      "connect close\n",
      "connect success\n",
      "connect close\n",
      "[(16, 3343, 509, 17, 840.91, '2020-11-14', 'East', 14295.47), (38, 7985, 762, 15, 190.86, '2022-08-07', 'East', 2862.9), (52, 7037, 674, 11, 586.55, '2023-11-28', 'East', 6452.049999999999), (56, 7420, 298, 16, 66.81, '2020-01-04', 'East', 1068.96)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py as hdf5\n",
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "\n",
    "path_file = \"sales_data.csv\"\n",
    "use_cols = ['customer_id', 'product_id', 'quantity', 'price']\n",
    "\n",
    "dtypes = {\n",
    "    'transaction_id' : 'int32' ,\n",
    "    'customer_id' : 'int32'  ,  \n",
    "    'product_id' : 'int16'  ,        \n",
    "    'quantity': 'int16',          \n",
    "    'price': 'float32',                  \n",
    "}\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_size_rows = 10\n",
    "data = pd.DataFrame() \n",
    "data_transaction = pd.DataFrame()\n",
    "\n",
    "def check_fun_error(fun):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return fun(*args, **kwargs)  \n",
    "        except Exception as e: \n",
    "            print(f\"❌ An error occurred in function '{fun.__name__}': {str(e)}\")\n",
    "            exit(1)  \n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def timing(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Temps d'exécution de {func.__name__}: {end_time - start_time:.4f} secondes\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def connect_to_db(func):\n",
    "    def wrapper(*args , **kwargs) :\n",
    "        try :\n",
    "            connection = sqlite3.connect('sales.db')\n",
    "            cursor = connection.cursor()\n",
    "            kwargs['connection'] = connection\n",
    "            kwargs['cursor'] = cursor\n",
    "            print(\"connect success\") \n",
    "            return func(*args , **kwargs)\n",
    "        except Exception as e :\n",
    "            print(f\"❌ An error occurred in function '{func.__name__}': {str(e)}\")\n",
    "        \n",
    "        finally :\n",
    "            print(\"connect close\") \n",
    "            connection.commit()\n",
    "            connection.close()\n",
    "    return wrapper\n",
    "            \n",
    "                 \n",
    "@check_fun_error  \n",
    "def load_file_with_chunks(path_file, chunk_size) -> None:\n",
    "    global data \n",
    "    try : \n",
    "        for ch in pd.read_csv(path_file, usecols= use_cols, dtype= dtypes  , chunksize=chunk_size):\n",
    "            data = pd.concat([data, ch], ignore_index=True).sample(frac=0.1  , random_state = 2)\n",
    "    finally : \n",
    "        if len(data) > 0:\n",
    "            print(\"✅ Data loaded successfully.\")\n",
    "        else:\n",
    "            print(\"⚠️ Something went wrong, data is empty.\")\n",
    "\n",
    "\n",
    "@check_fun_error\n",
    "@timing\n",
    "def convert_to_feather_or_parquet(type_convert) : \n",
    "    global data\n",
    "    type_convert = type_convert.lower()\n",
    "    if type_convert == \"feather\":\n",
    "        print(\"You convert to feather \")\n",
    "        return data.to_feather('data.feather')\n",
    "    if type_convert == \"parquet\" :\n",
    "        print(\"You convert to parquet  \") \n",
    "        return data.to_parquet('data.parquet')\n",
    "    \n",
    "@check_fun_error\n",
    "@timing\n",
    "def read_files_feather_parquet_csv(type_file):\n",
    "    type_file = type_file.lower()  \n",
    "    if type_file == \"feather\":\n",
    "        print(\"You are reading a Feather file\")\n",
    "        return pd.read_feather(\"data.feather\")\n",
    "    elif type_file == \"parquet\":\n",
    "        print(\"You are reading a Parquet file\")\n",
    "        return pd.read_parquet(\"data.parquet\")\n",
    "    elif type_file == \"csv\":\n",
    "        print(\"You are reading a CSV file\")\n",
    "        return pd.read_csv(\"sales_data.csv\")\n",
    "    else:\n",
    "        print(\"Unsupported file type. Please choose from 'feather', 'parquet', or 'csv'.\")\n",
    "        return None       \n",
    "\n",
    "@check_fun_error\n",
    "def compare_size_files():\n",
    "    # Obtenir la taille des fichiers en octets\n",
    "    file_size_csv = os.path.getsize('sales_data.csv')\n",
    "    file_size_feather = os.path.getsize('data.feather')\n",
    "    file_size_parquet = os.path.getsize('data.parquet')\n",
    "    \n",
    "    # Afficher les tailles des fichiers\n",
    "    print(f\"Taille du fichier CSV: {file_size_csv:.2f} octets\")\n",
    "    print(f\"Taille du fichier Feather: {file_size_feather:.2f} octets\")\n",
    "    print(f\"Taille du fichier Parquet: {file_size_parquet:.2f} octets\")\n",
    "\n",
    "    # Comparaison des tailles pour déterminer le fichier le plus volumineux\n",
    "    if file_size_csv > file_size_feather and file_size_csv > file_size_parquet:\n",
    "        print(\"Le fichier CSV est le plus volumineux.\")\n",
    "    elif file_size_feather > file_size_csv and file_size_feather > file_size_parquet:\n",
    "        print(\"Le fichier Feather est le plus volumineux.\")\n",
    "    elif file_size_parquet > file_size_csv and file_size_parquet > file_size_feather:\n",
    "        print(\"Le fichier Parquet est le plus volumineux.\")\n",
    "    else:\n",
    "        print(\"Plusieurs fichiers ont des tailles équivalentes.\")\n",
    "        \n",
    "@check_fun_error\n",
    "def file_hdf5( file_name , data , data_transaction_supp_100) :\n",
    "    try : \n",
    "        with hdf5.File(file_name, \"w\") as hd5_file:\n",
    "            hd5_file.create_dataset('sales_sample', data=data)\n",
    "            hd5_file.create_dataset('sales_high_transaction', data=data_transaction_supp_100)\n",
    "        print(\"ok\")\n",
    "    except Exception as e :\n",
    "        print(f\"❌ An error occurred in function : {str(e)}\")   \n",
    "    pass\n",
    "\n",
    "@check_fun_error\n",
    "def read_first_five_rows_from_hdf5(file_name):\n",
    "    with hdf5.File(file_name, 'r') as hdf:\n",
    "        print(hdf['sales_sample'][0:5])    \n",
    "        \n",
    "@check_fun_error       \n",
    "def read_file_from_rows (file_path , chunk_size) : \n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        yield chunk \n",
    "\n",
    "@check_fun_error\n",
    "def combine_and_calcul_total() : \n",
    "    global data_transaction\n",
    "    for chunk in read_file_from_rows('sales_data.csv'  , chunk_size_rows):\n",
    "        data_transaction = pd.concat([data_transaction , chunk[chunk.quantity > 10]] , ignore_index=True)\n",
    "    \n",
    "    data_transaction['total_value'] = data_transaction['price'] * data_transaction['quantity']\n",
    "    total = data_transaction['total_value'].sum()\n",
    "    print(\"Data : \\n\", data_transaction.head(10))\n",
    "    print(\"Total : \\n\", total)\n",
    "\n",
    "@check_fun_error\n",
    "@connect_to_db\n",
    "def convert_csv_to_sql_db(connection , cursor = None):\n",
    "    data_transaction.to_sql('sales' , connection , if_exists='replace' , index=False )\n",
    "\n",
    "@check_fun_error\n",
    "@connect_to_db\n",
    "def select_transaction_group_by_eur(cursor ,connection = None ):\n",
    "    stm = cursor.execute(\"\"\" SELECT * FROM sales WHERE region='East' and price > 50   \"\"\")\n",
    "    data = stm.fetchall()\n",
    "    return  data\n",
    "      \n",
    "        \n",
    "if len(data)  ==  0 :\n",
    "    load_file_with_chunks(path_file=path_file, chunk_size=chunk_size)\n",
    "\n",
    "data_transaction_supp_100 = data[ data.price > 100 ]\n",
    "    \n",
    "# print(\"#\" * 60 )\n",
    "# convert_to_feather_or_parquet(\"feather\")\n",
    "\n",
    "# print(\"#\" * 60 )\n",
    "# convert_to_feather_or_parquet(\"parquet\")\n",
    "\n",
    "# print(\"#\" * 60 )\n",
    "# file_hdf5('sales_data.h5' ,  data=data , data_transaction_supp_100=data_transaction_supp_100)\n",
    "# read_first_five_rows_from_hdf5('sales_data.h5')\n",
    "# df = read_files_feather_parquet_csv(\"feather\")\n",
    "\n",
    "\n",
    "# print(\"#\" * 60 )\n",
    "# df = read_files_feather_parquet_csv(\"parquet\")\n",
    "# print(\"#\" * 60 )\n",
    "# df = read_files_feather_parquet_csv(\"csv\")\n",
    "\n",
    "# print(df.head(10))\n",
    "\n",
    "# compare_size_files()\n",
    "\n",
    "# print(data.info())\n",
    "\n",
    "combine_and_calcul_total()\n",
    "convert_csv_to_sql_db()\n",
    "print(select_transaction_group_by_eur())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = {\n",
    "    'TransactionID': [1, 2, 3, 4, 5],\n",
    "    'Prix': [50, 150, 80, 120, 200],\n",
    "    'Quantité': [1, 2, 1, 3, 1]\n",
    "}\n",
    "normal_data = pd.DataFrame(data)\n",
    "\n",
    "# Convertir les types avec numpy\n",
    "normal_data['TransactionID'] = normal_data['TransactionID'].astype(np.int32) \n",
    "normal_data['Prix'] = normal_data['Prix'].astype(np.float64)\n",
    "normal_data['Quantité'] = normal_data['Quantité'].astype(np.int32)\n",
    "\n",
    "# Filtrer les transactions dont le prix est supérieur à 100\n",
    "data_transaction_supp_100 = normal_data[normal_data['Prix'] > 100]\n",
    "\n",
    "# Créer un fichier HDF5 et y stocker les deux tables\n",
    "with h5py.File(\"sales_data.h5\", \"w\") as hd5_file:\n",
    "    hd5_file.create_dataset('sales_sample', data=normal_data)\n",
    "    hd5_file.create_dataset('sales_high_transaction', data=data_transaction_supp_100)\n",
    "\n",
    "with h5py.File('sales_data.h5', 'r') as hdf:\n",
    "    # Lire les données du table 'sales_sample'\n",
    "    print(hdf['sales_sample'][0:5]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
