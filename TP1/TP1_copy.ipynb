{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5402ea44-07a2-4482-bfd8-b89d37eb4245",
   "metadata": {},
   "source": [
    "##### Rôle des Décorateurs\n",
    "\n",
    "#### 1. **`check_error`** :\n",
    "   - **Rôle** : Intercepte et gère les exceptions qui se produisent lors de l'exécution de la fonction décorée, en affichant un message d'erreur avec le nom de la fonction et le message de l'exception.\n",
    "\n",
    "#### 2. **`timing`** :\n",
    "   - **Rôle** : Mesure le temps d'exécution de la fonction décorée et affiche la durée de l'exécution en secondes.\n",
    "\n",
    "#### 3. **`connect_db`** :\n",
    "   - **Rôle** : Gère la connexion à une base de données SQLite. Il établit la connexion, crée un curseur, passe la connexion et le curseur à la fonction décorée, puis s'assure que la connexion est validée et fermée après l'exécution de la fonction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384eea45-6d75-4778-83aa-bb8821762f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declarations des decorateurs selon les besoins:\n",
    "def check_error(function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return function(*args, **kwargs)  \n",
    "        except Exception as e: \n",
    "            print(\"Fonction d'erreur: {}, message: {}\".format(function.__name__,str(e)))  \n",
    "    return wrapper\n",
    "\n",
    "def timing(function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start:float = time.time()\n",
    "        result = function(*args, **kwargs)\n",
    "        end:float = time.time()\n",
    "        print(\"Temps d'exécution de {}: {:.4f} secondes\".format(function.__name__ , end - start))\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def connect_db(function):\n",
    "    def wrapper(*args , **kwargs) :\n",
    "        try :\n",
    "            connection:sqlite3.connect = sqlite3.connect(kwargs[\"table\"])\n",
    "            cursor:connection.cursor = connection.cursor()\n",
    "            kwargs['connection'] = connection\n",
    "            kwargs['cursor'] = cursor\n",
    "            print(\"connection avec succes\") \n",
    "            return function(*args , **kwargs)\n",
    "        except Exception as e :\n",
    "            print(\"Fonction d'erreur: {}, message: {}\".format(function.__name__,str(e)))\n",
    "        \n",
    "        finally :\n",
    "            connection.commit()\n",
    "            connection.close()\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf1b6f6-9fbb-4ab4-a3e3-16e4f5f1c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donnés chargé savec succe!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 309190 to 194758\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   customer_id  10000 non-null  uint32 \n",
      " 1   product_id   10000 non-null  uint16 \n",
      " 2   quantity     10000 non-null  uint8  \n",
      " 3   price        10000 non-null  float32\n",
      "dtypes: float32(1), uint16(1), uint32(1), uint8(1)\n",
      "memory usage: 185.5 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py as hdf5\n",
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "from typing import List, Dict\n",
    "\n",
    "#1. Échantillonnage et Sous-ensemble de Données:\n",
    "@check_error  \n",
    "def load_csv_file_chunks(path_file:str, use_cols:List[str], dtypes:Dict[str,str], chunk_size:int,fraction:float=0.01)->pd.DataFrame:\n",
    "    data:pd.DataFrame = pd.DataFrame()\n",
    "    for ch in pd.read_csv(path_file , usecols= use_cols, dtype= dtypes ,chunksize=chunk_size):\n",
    "        data = pd.concat([data, ch], ignore_index=True)\n",
    "    print(\"donnés chargé savec succe!\")\n",
    "    return data.sample(frac=fraction, random_state = 2)\n",
    "\n",
    "data_sales:pd.DataFrame = pd.DataFrame()\n",
    "cols:List[str] = [\"customer_id\",\"product_id\",\"quantity\",\"price\"]\n",
    "dtypes:Dict[str,str] = {\n",
    "    'customer_id' : 'uint32',  \n",
    "    'product_id' : 'uint16',        \n",
    "    'quantity': 'uint8',          \n",
    "    'price': 'float32',                  \n",
    "}\n",
    "chunk_size:int = 100000\n",
    "data_sales = load_csv_file_chunks('sales_data.csv',cols,dtypes,chunk_size)\n",
    "print(data_sales.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3962a63c-a281-48c8-b9d7-251ec63b47f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnés converté au feather\n",
      "Donnés converté au parquet\n",
      "Taille du fichier Parquet: 226743.00 octets\n",
      "Taille du fichier CSV: 205988.00 octets\n",
      "Taille du fichier Feather: 158714.00 octets\n",
      "########################################\n",
      "Chargement d'un fichier  csv\n",
      "Temps d'exécution de read_files_feather_parquet_csv: 0.0387 secondes\n",
      "########################################\n",
      "Chargement d'un fichier  feather\n",
      "Temps d'exécution de read_files_feather_parquet_csv: 0.0145 secondes\n",
      "########################################\n",
      "Chargement d'un fichier  parquet\n",
      "Temps d'exécution de read_files_feather_parquet_csv: 0.0161 secondes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Conversion en Formats de Fichiers Efficaces:\n",
    "@check_error\n",
    "def convert_to_feather_or_parquet(data:pd.DataFrame,type_convert_feather:bool=True)->None:  \n",
    "    if type_convert_feather == True:\n",
    "        print(\"Donnés converté au feather\")\n",
    "        data.to_feather('data.feather')\n",
    "    if type_convert_feather == False:\n",
    "        print(\"Donnés converté au parquet\") \n",
    "        data.to_parquet('data.parquet')\n",
    "\n",
    "@check_error\n",
    "@timing\n",
    "def read_files_feather_parquet_csv(file_name:str)->any:\n",
    "    choice:List[str] = file_name.split(\".\")  \n",
    "\n",
    "    match choice[-1]:\n",
    "        case \"feather\":\n",
    "            print(\"Chargement d'un fichier \",choice[-1])\n",
    "            return pd.read_feather(file_name)\n",
    "        case \"parquet\":\n",
    "            print(\"Chargement d'un fichier \",choice[-1])\n",
    "            return pd.read_parquet(file_name)\n",
    "        case \"csv\":\n",
    "            print(\"Chargement d'un fichier \",choice[-1])\n",
    "            return pd.read_csv(file_name)\n",
    "        case _:\n",
    "            print(\"Ce type n'est pas un type (csv,feather ou paquet).\")\n",
    "            return None\n",
    "#conveersion csv:\n",
    "data_sales.to_csv('data.csv', index=False)\n",
    "#conversion feather:\n",
    "convert_to_feather_or_parquet(data_sales,type_convert_feather=True)\n",
    "#conversion parquet:\n",
    "convert_to_feather_or_parquet(data_sales,type_convert_feather=False)\n",
    "\n",
    "#comparaison des tailles ( parquet, feateher et csv):\n",
    "# Obtenir la taille des fichiers en octets\n",
    "file_size_csv = os.path.getsize('data.csv')\n",
    "file_size_feather = os.path.getsize('data.feather')\n",
    "file_size_parquet = os.path.getsize('data.parquet')\n",
    "\n",
    "print(\"Taille du fichier Parquet: {:.2f} octets\".format(file_size_parquet))\n",
    "print(\"Taille du fichier CSV: {:.2f} octets\".format(file_size_csv))\n",
    "print(\"Taille du fichier Feather: {:.2f} octets\".format(file_size_feather))\n",
    "\n",
    "\n",
    "#mesure du temps de chargement des fichiers:\n",
    "print(\"#\"*40)\n",
    "csv_data:pd.DataFrame = read_files_feather_parquet_csv(\"data.csv\")\n",
    "print(\"#\"*40)\n",
    "feather_data:pd.DataFrame = read_files_feather_parquet_csv(\"data.feather\")\n",
    "print(\"#\"*40)\n",
    "parquet_data:pd.DataFrame = read_files_feather_parquet_csv(\"data.parquet\")\n",
    "csv_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "8d27fcad-3b7d-4a31-9b95-931a4cb18ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.82370000e+04 5.87200000e+03 5.00000000e+00 2.71320007e+02]\n",
      " [2.16230000e+04 4.43700000e+03 2.00000000e+00 2.84260010e+02]\n",
      " [4.53030000e+04 6.76700000e+03 1.00000000e+01 3.16850006e+02]\n",
      " [2.34910000e+04 5.13400000e+03 9.00000000e+00 8.67500000e+01]\n",
      " [1.68730000e+04 6.68600000e+03 1.00000000e+00 1.84309998e+02]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "#Utilisation de HDF5:\n",
    "@check_error\n",
    "def file_hdf5( file_name:str , data:pd.DataFrame , table_name:str)->None:\n",
    "        with hdf5.File(file_name, \"a\") as hd5_file:\n",
    "            hd5_file.create_dataset(table_name, data=data.values)\n",
    "#stockez l'échantillon de données dans une table appelée sales_sample.\n",
    "\n",
    "file_hdf5(\"sales_data.h5\", data_sales, 'sales_sample')\n",
    "#Ajoutez une deuxième table contenant les transactions dont le prix est supérieur à 100 DH.\n",
    "data_transaction_supp_100 = data_sales[data_sales['price'] > 100]\n",
    "file_hdf5(\"sales_data.h5\", data_transaction_supp_100, 'sales_supp_100')\n",
    "#Lire les données du table 'sales_sample' et affich 5 premier :\n",
    "with h5py.File('sales_data.h5', 'r') as hdf:\n",
    "    print(hdf['sales_sample'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "425435ef-c536-4914-87f4-267101608535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donnees filtrer : 0\n",
      "total des ventes:  0.0\n"
     ]
    }
   ],
   "source": [
    "#4. Lecture par Morceaux:\n",
    "@check_error\n",
    "def read_csv_file_by_parts(file_name:str,parts:int):\n",
    "    for part in pd.read_csv(file_name, chunksize=parts):\n",
    "        yield part\n",
    "#Lisez le fichier sales_data.csv par morceaux de 100 000 lignes et filtrez les transactions ayant une quantité supérieure à 10.\n",
    "#et combinaison des dataframe filtrer.\n",
    "data_transaction:pd.DataFrame = pd.DataFrame()\n",
    "for part in read_csv_file_by_parts(\"sales_data.csv\",100000):\n",
    "    data_transaction = pd.concat([data_transaction , part[part['quantity'] > 10]] , ignore_index=True)\n",
    "print(\"donnees filtrer :\", len(data_transaction))\n",
    "#calculez le total des ventes (quantité * prix) pour ces transactions.\n",
    "data_transaction['total_value'] = data_transaction['price'] * data_transaction['quantity']\n",
    "total = data_transaction['total_value'].sum()\n",
    "print(\"total des ventes: \",total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "d8d4d82a-07a7-456d-a521-9a1709dfc65f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection avec succes\n",
      "donnes csv to sql est bien fait!\n",
      "connection avec succes\n",
      "donnees region 'Europe' et price supp a 50 :\n",
      "        transaction_id  customer_id  product_id  quantity   price  \\\n",
      "0                    3        76821        3079         5  473.96   \n",
      "1                    4        54887        9037         4  466.34   \n",
      "2                    9        44132        1056         5  405.97   \n",
      "3                   14        64821        3710         3  438.17   \n",
      "4                   16        59736        4594         2  433.96   \n",
      "...                ...          ...         ...       ...     ...   \n",
      "153039          999950        38660        6416         7   78.04   \n",
      "153040          999959         7434        9012         4  474.91   \n",
      "153041          999962        38987        5372         1  474.39   \n",
      "153042          999989        56292        4593         4  352.91   \n",
      "153043          999998        54332        3891        10  186.24   \n",
      "\n",
      "       transaction_date  region  \n",
      "0            2023-09-20  Europe  \n",
      "1            2020-12-11  Europe  \n",
      "2            2020-06-16  Europe  \n",
      "3            2021-06-04  Europe  \n",
      "4            2023-11-21  Europe  \n",
      "...                 ...     ...  \n",
      "153039       2023-07-04  Europe  \n",
      "153040       2022-02-09  Europe  \n",
      "153041       2020-12-21  Europe  \n",
      "153042       2023-09-23  Europe  \n",
      "153043       2023-03-25  Europe  \n",
      "\n",
      "[153044 rows x 7 columns]\n",
      "total transaction:  231748549.08000004\n"
     ]
    }
   ],
   "source": [
    "#5. Chargement dans une Base de Données:\n",
    "@check_error\n",
    "@connect_db\n",
    "def copy_csv_to_sql_db(connection:sqlite3.connect , data:pd.DataFrame, table:str ,cursor = None)->None:\n",
    "    data.to_sql(table, connection , if_exists='replace' , index=False )\n",
    "    print(\"donnes csv to sql est bien fait!\")\n",
    "\n",
    "\n",
    "@check_error\n",
    "@connect_db\n",
    "def select_transaction_group_by_eur(cursor , connection:sqlite3.connect = None , table:str=None) :\n",
    "    stm = cursor.execute(\"\"\" SELECT * FROM sales WHERE region='Europe' and price > 50   \"\"\")\n",
    "    data = stm.fetchall()\n",
    "    return  data\n",
    "\n",
    "#Créez une base de données SQLite (sales.db) et chargez l'intégralité du fichier\n",
    "#sales_data.csv dans une table appelée sales.\n",
    "copy_csv_to_sql_db(table=\"sales\",data=csv_data)\n",
    "#Exécutez une requête SQL pour extraire les transactions ayant eu lieu dans la région\n",
    "#\"Europe\" et dont le prix est supérieur à 50 DH.\n",
    "data_region_price:pd.DataFrame = pd.DataFrame(select_transaction_group_by_eur(table=\"sales\"), columns=csv_data.columns)\n",
    "print('donnees region \\'Europe\\' et price supp a 50 :')\n",
    "print(data_region_price)\n",
    "#Calculez le total des ventes pour ces transactions et affichez le résultat.\n",
    "data_region_price['total_value'] = data_region_price['price'] * data_region_price['quantity']\n",
    "total = data_region_price['total_value'].sum()\n",
    "print(\"total transaction: \",total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "7906ab8d-be60-4a73-93f1-2f7cfdc6607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection avec succes\n",
      "             0      1     2   3       4           5       6\n",
      "0            3  76821  3079   5  473.96  2023-09-20  Europe\n",
      "1            4  54887  9037   4  466.34  2020-12-11  Europe\n",
      "2            9  44132  1056   5  405.97  2020-06-16  Europe\n",
      "3           14  64821  3710   3  438.17  2021-06-04  Europe\n",
      "4           16  59736  4594   2  433.96  2023-11-21  Europe\n",
      "...        ...    ...   ...  ..     ...         ...     ...\n",
      "153039  999950  38660  6416   7   78.04  2023-07-04  Europe\n",
      "153040  999959   7434  9012   4  474.91  2022-02-09  Europe\n",
      "153041  999962  38987  5372   1  474.39  2020-12-21  Europe\n",
      "153042  999989  56292  4593   4  352.91  2023-09-23  Europe\n",
      "153043  999998  54332  3891  10  186.24  2023-03-25  Europe\n",
      "\n",
      "[153044 rows x 7 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "89d65fc7-51d2-4473-b6d6-f7f9efda4c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.  50.   1.]\n",
      " [  2. 150.   2.]\n",
      " [  3.  80.   1.]\n",
      " [  4. 120.   3.]\n",
      " [  5. 200.   1.]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = {\n",
    "    'TransactionID': [1, 2, 3, 4, 5],\n",
    "    'Prix': [50, 150, 80, 120, 200],\n",
    "    'Quantité': [1, 2, 1, 3, 1]\n",
    "}\n",
    "normal_data = pd.DataFrame(data)\n",
    "\n",
    "# Convertir les types avec numpy\n",
    "normal_data['TransactionID'] = normal_data['TransactionID'].astype(np.int32) \n",
    "normal_data['Prix'] = normal_data['Prix'].astype(np.float64)\n",
    "normal_data['Quantité'] = normal_data['Quantité'].astype(np.int32)\n",
    "\n",
    "# Filtrer les transactions dont le prix est supérieur à 100\n",
    "data_transaction_supp_100 = normal_data[normal_data['Prix'] > 100]\n",
    "\n",
    "# Créer un fichier HDF5 et y stocker les deux tables\n",
    "with h5py.File(\"sales_data.h5\", \"w\") as hd5_file:\n",
    "    hd5_file.create_dataset('sales_sample', data=normal_data)\n",
    "    hd5_file.create_dataset('sales_high_transaction', data=data_transaction_supp_100)\n",
    "\n",
    "with h5py.File('sales_data.h5', 'r') as hdf:\n",
    "    # Lire les données du table 'sales_sample'\n",
    "    print(hdf['sales_sample'][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "932e63d4-c7e9-49d1-8000-c0b6ad631f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  customer_id  product_id  quantity   price transaction_date  \\\n",
      "0               1        15796         111         7  336.48       2023-10-03   \n",
      "1               2          861        3214         3  471.86       2022-06-01   \n",
      "2               3        76821        3079         5  473.96       2023-09-20   \n",
      "3               4        54887        9037         4  466.34       2020-12-11   \n",
      "4               5         6266        7572         7   39.62       2023-07-04   \n",
      "\n",
      "          region  \n",
      "0  North America  \n",
      "1      Australia  \n",
      "2         Europe  \n",
      "3         Europe  \n",
      "4  North America  \n",
      "Fichier 'sales_data.csv' sauvegardé avec succès.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "# Paramètres pour la génération de données\n",
    "num_rows = 1_000_000 # 1 million de lignes\n",
    "regions = ['North America', 'Europe', 'Asia', 'South America', 'Africa', 'Australia']\n",
    "start_date = datetime(2020, 1, 1) # Date de début pour les transactions\n",
    "end_date = datetime(2023, 12, 31) # Date de fin pour les transactions\n",
    "# Génération des données\n",
    "np.random.seed(42) # Pour la reproductibilité\n",
    "# transaction_id : Identifiant unique\n",
    "transaction_ids = np.arange(1, num_rows + 1)\n",
    "# customer_id : Identifiant client aléatoire entre 1 et 100 000\n",
    "customer_ids = np.random.randint(1, 100_001, num_rows)\n",
    "# product_id : Identifiant produit aléatoire entre 1 et 10 000\n",
    "product_ids = np.random.randint(1, 10_001, num_rows)\n",
    "# quantity : Quantité aléatoire entre 1 et 10\n",
    "quantities = np.random.randint(1, 11, num_rows)\n",
    "# price : Prix aléatoire entre 10 et 500 (avec 2 décimales)\n",
    "prices = np.round(np.random.uniform(10, 500, num_rows), 2)\n",
    "# transaction_date : Date aléatoire entre start_date et end_date\n",
    "date_range = (end_date - start_date).days\n",
    "transaction_dates = [start_date + timedelta(days=np.random.randint(0, date_range)) for _ in range(num_rows)]\n",
    "# region : Région aléatoire parmi la liste définie\n",
    "regions_data = np.random.choice(regions, num_rows)\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame({\n",
    "'transaction_id': transaction_ids,\n",
    "'customer_id': customer_ids,\n",
    "'product_id': product_ids,\n",
    "'quantity': quantities,\n",
    "'price': prices,\n",
    "'transaction_date': transaction_dates,\n",
    "'region': regions_data\n",
    "})\n",
    "# Affichage des 5 premières lignes\n",
    "print(df.head())\n",
    "# Sauvegarde du DataFrame dans un fichier CSV\n",
    "df.to_csv('sales_data.csv', index=False)\n",
    "print(\"Fichier 'sales_data.csv' sauvegardé avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127f817-ce3b-4bf1-a683-7c6ad7890825",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_id,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
